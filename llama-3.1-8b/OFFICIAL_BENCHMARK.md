# Model evaluations


| Category        | Benchmark                         | LLaMA 3.1 8B |
|-----------------|-----------------------------------|--------------|
| General         | MMLU (0-shot, CoT)                | 73.0         |
| General         | MMLU PRO (5-shot, CoT)            | 48.3         |
| General         | IFEval                            | 80.4         |
| Code            | HumanEval (0-shot)                | 72.6         |
| Code            | MBPP EvalPlus (base, 0-shot)      | 72.8         |
| Math            | GSM8K (8-shot, CoT)               | 84.5         |
| Math            | MATH (0-shot, CoT)                | 51.9         |
| Reasoning       | ARC Challenge (0-shot)            | 83.4         |
| Reasoning       | GPQA (0-shot, CoT)                | 32.8         |
| Tool use        | BFCL                              | 76.1         |
| Tool use        | Nexus                             | 38.5         |
| Long context    | ZeroSCROLLS/QUALITY               | 81.0         |
| Long context    | InfiniteBench / En.MC             | 65.1         |
| Long context    | NIH / Multi-needle                | 98.8         |
| Multilingual    | Multilingual MGSM (0-shot)        | 68.9         |

# References 
- https://ai.meta.com/blog/meta-llama-3-1/
