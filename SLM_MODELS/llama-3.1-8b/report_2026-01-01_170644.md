# Llama 3.1 8B Benchmark Report (Ollama)

## Run Info

- Date: 2026-01-01 17:20:39
- Model: `llama3.1:8b`
- Runs per prompt: `5`
- Prompts file: `llama-3.1-8b/prompts.jsonl`
- Raw results: `llama-3.1-8b/results_2026-01-01_170644.jsonl`

## Environment

- ollama version is 0.13.5
- macOS:
```
ProductName:		macOS
ProductVersion:		26.2
BuildVersion:		25C56
```
- CPU: Apple M4
- RAM: 16.0 GiB

## Overall Summary

| metric | n | mean | p50 | p95 | min | max |
|---|---:|---:|---:|---:|---:|---:|
| ttft_s | 25 | 0.6661 | 0.1075 | 0.3510 | 0.1035 | 13.3535 |
| wall_s | 25 | 4.4059 | 2.2632 | 10.2452 | 1.0686 | 15.4358 |
| gen_toks_per_s | 25 | 20.8008 | 20.8931 | 21.5290 | 19.0289 | 21.5433 |
| prompt_toks_per_s | 25 | 527.5747 | 472.5291 | 799.2532 | 6.4583 | 799.3851 |

## Per-Prompt Summary (p50)

| prompt_id | ttft_s | wall_s | gen_toks_per_s | prompt_toks_per_s |
|---|---:|---:|---:|---:|
| code_1 | 0.1085 | 2.2626 | 20.8933 | 799.0671 |
| format_1 | 0.1071 | 1.0689 | 21.5210 | 695.8319 |
| reasoning_1 | 0.1083 | 3.7651 | 20.5533 | 772.3446 |
| short_qa_1 | 0.1044 | 1.9710 | 21.0700 | 471.2089 |
| short_qa_2 | 0.1068 | 10.1176 | 20.4365 | 409.2546 |

## Notes

- `ttft_s` is often much higher on the first run due to cold start (model load / cache warm-up).
- For apples-to-apples comparisons, do a warm-up run or discard the first run per prompt.
- Increase `--num_predict` to benchmark longer generations; increase `--runs` for tighter confidence.
